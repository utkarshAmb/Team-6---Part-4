{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-Speech (TTS) Synthesizer sub-module\n",
    "\n",
    "* Receives processed text from the NLP Engine\n",
    "* Gather Requirement from User Personalization Module.\n",
    "* Generates audio output that mimics natural speech, taking into account user-specific settings adjusted for clarity and ease of understanding.\n",
    "* Outputs the audio directly to the user interface, facilitating real-time interaction with the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gtts in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (2.5.4)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lanqingcui/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "#change the directory to run successfully at different end\n",
    "os.chdir('/Users/lanqingcui/Desktop')\n",
    "\n",
    "def text_to_speech(text, filename=\"output.mp3\", lang='en', slow=False):\n",
    "    \"\"\"Converts the given text to speech and saves it as an MP3 file with a custom name.\"\"\"\n",
    "    tts = gTTS(text=text, lang=lang, slow=slow)\n",
    "    tts.save(filename)\n",
    "    os.system(f\"afplay {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text_to_speech(\"Hello World! Welcome to our demonstration of text to speech. This is one simple example of easiest TTS. \", \"greeting.mp3\", lang='en',slow=True)\n",
    "    text_to_speech(\"Hello World! Welcome to our demonstration of text to speech. This is one simple example of easiest TTS. \", \"greeting2.mp3\", lang='en',slow=False)\n",
    "    text_to_speech(\"大家好呀！这是中文版测试，你今天过的怎么样? \", \"chinese_greeting.mp3\", lang='zh-CN',slow=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AI models: \n",
    "1. Fast\n",
    "2. High Quality\n",
    "3. With emotions\n",
    "\n",
    "Find Dataset: \n",
    "    Publicly Available Emotional Speech Dataset (ESD) for Speech Synthesis and Voice Conversion\n",
    "       Dataset contains 10 English speakers with 5 emotional states (neutral, happy, angry, sad and surprise).\n",
    "\n",
    "Preprocess data and do some training\n",
    "    Record necessary informations and normalize data\n",
    "    Sample Method:\n",
    "        1) extracts phones and feature from file\n",
    "        2) computes and saves:\n",
    "            2.1 pitch\n",
    "            2.2 mel-spectrogram\n",
    "            2.3 energy\n",
    "            2.4 durations\n",
    "        3) write in the data and paths\n",
    "            :param basename: str, filename without extension\n",
    "            :param tg_path: Path, path to .TextGrid file\n",
    "            :param wav_path: Path, path to .wav file\n",
    "            :param txt_path: Path, path to .txt file\n",
    "\n",
    "Impliment model and train the model on the data\n",
    "    FastSpeech2\n",
    "    \n",
    "Check the Results of generation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def __getitem__(self, idx: int) -> dict:\n",
    "        speaker_id = self.speaker_id[idx]\n",
    "        file_id = self.file_id[idx]\n",
    "        emotion_id = self.emotion_id[idx]\n",
    "        phone = np.array(\n",
    "            [self.phones_mapping[i] for i in self.text[idx][1:-1].split(\" \")]\n",
    "        )\n",
    "        basename = f\"{speaker_id}_{file_id}_{emotion_id}\"\n",
    "\n",
    "        if self.n_egemap_features > 0:\n",
    "            egemap_feature = np.load(\n",
    "                str(Path(self.preprocessed_path) / \"egemap\" / f\"{basename}.npy\"),\n",
    "                allow_pickle=True,\n",
    "            )[: self.n_egemap_features]\n",
    "        else:\n",
    "            egemap_feature = None\n",
    "        mel = np.load(\n",
    "            str(Path(self.preprocessed_path) / \"mel\" / f\"{basename}.npy\"),\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "        pitch = np.load(\n",
    "            str(Path(self.preprocessed_path) / \"pitch\" / f\"{basename}.npy\"),\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "        energy = np.load(\n",
    "            str(Path(self.preprocessed_path) / \"energy\" / f\"{basename}.npy\"),\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "        duration = np.load(\n",
    "            str(Path(self.preprocessed_path) / \"duration\" / f\"{basename}.npy\"),\n",
    "            allow_pickle=True,\n",
    "        )\n",
    "\n",
    "        assert duration.shape == phone.shape, (\n",
    "            f\"Duration and phone shapes do not match. Phone shape {phone.shape}, \"\n",
    "            f\"duration: {duration.shape} for sample: {basename}.\"\n",
    "        )\n",
    "\n",
    "        sample = {\n",
    "            \"id\": basename,\n",
    "            \"speaker\": speaker_id,\n",
    "            \"emotion\": emotion_id,\n",
    "            \"mel\": mel,\n",
    "            \"pitch\": pitch,\n",
    "            \"energy\": energy,\n",
    "            \"duration\": duration,\n",
    "            \"text\": phone,\n",
    "            \"egemap_feature\": egemap_feature,\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    # Set Preprocess Parameters\n",
    "    n_threads: int = 16  # n_threads to parallel process utterance\n",
    "    include_empty_intervals: bool = (\n",
    "        True  # if True silence will be loaded from .TextGrid\n",
    "    )\n",
    "\n",
    "    mel_fmin: int = 0\n",
    "    mel_fmax: int = 8000\n",
    "    hop_length: int = 192\n",
    "    stft_length: int = 768\n",
    "    sample_rate: int = 16000\n",
    "    window_length: int = 768\n",
    "    n_mel_channels: int = 80\n",
    "\n",
    "    raw_data_path: Path = \"/app/data/data/ssw_esd\"\n",
    "    val_ids_path: Path = \"/app/data/val_ids.txt\"\n",
    "    test_ids_path: Path = \"/app/data/test_ids.txt\"\n",
    "    preprocessed_data_path: Path = Path(\"/app/data/preprocessed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Read and trim wav files\n",
    "        wav = torchaudio.load(wav_path)[0].cpu().numpy().squeeze(0)\n",
    "        wav = wav[\n",
    "            int(self.config.sample_rate * start) : int(self.config.sample_rate * end)\n",
    "        ].astype(np.float32)\n",
    "        speaker_idx, filename_idx, emotion_idx = basename.split(\"_\")\n",
    "\n",
    "\n",
    "# Compute pitch for certain audio\n",
    "        pitch, t = pw.dio( #Distributed Inline-filter Operation (DIO) algorithm.\n",
    "            wav.astype(np.float64),\n",
    "            self.config.sample_rate,\n",
    "            frame_period=self.config.hop_in_ms * 1000,\n",
    "        )\n",
    "        #Refined Pitch Estimation\n",
    "        pitch = pw.stonemask(wav.astype(np.float64), pitch, t, self.config.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Speech, self).__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.n_emotions = config.n_emotions\n",
    "\n",
    "        self.variance_adaptor = VarianceAdaptor(config)\n",
    "\n",
    "        self.emotion_emb = nn.Embedding(\n",
    "            config.n_emotions, config.emotion_emb_hidden_size\n",
    "        )\n",
    "        self.mel_linear = nn.Linear(\n",
    "            config.transformer_decoder_hidden, config.n_mel_channels\n",
    "        )\n",
    "        self.speaker_emb = nn.Embedding(\n",
    "            config.n_speakers + 1, config.speaker_emb_hidden_size\n",
    "        )\n",
    "\n",
    "        # Advanced Emotion Conditioning\n",
    "        self.conditional_cross_attention = config.conditional_cross_attention\n",
    "        self.conditional_layer_norm_usage = config.conditional_layer_norm\n",
    "        self.stack_speaker_with_emotion_embedding = (\n",
    "            config.stack_speaker_with_emotion_embedding\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, device, batch_dict) -> dict:\n",
    "\n",
    "        # Retrieves emotion and speaker embeddings based on the IDs\n",
    "        emotion_embedding = self.emotion_emb(batch_dict[\"emotions\"].to(device))\n",
    "        speaker_embedding = self.speaker_emb(batch_dict[\"speakers\"].to(device))\n",
    "\n",
    "        #if there is need of Conditional Processing\n",
    "        if self.conditional_cross_attention or self.conditional_layer_norm_usage:\n",
    "            encoder_output, encoder_attention = self.encoder(\n",
    "                batch_dict[\"texts\"].to(device),\n",
    "                src_masks.to(device),\n",
    "                speaker_emotion_embedding=emotion_embedding,\n",
    "            )\n",
    "        else:\n",
    "            encoder_output, encoder_attention = self.encoder(\n",
    "                batch_dict[\"texts\"].to(device),\n",
    "                src_masks.to(device),\n",
    "                speaker_emotion_embedding=None,\n",
    "            )\n",
    "\n",
    "        #checks if the speaker and emotion embeddings are not meant to be stacked together before being added to the encoder output.\n",
    "        if not self.stack_speaker_with_emotion_embedding:\n",
    "            max_src_len = torch.max(batch_dict[\"text_lens\"]).item()\n",
    "            encoder_output = (\n",
    "                encoder_output\n",
    "                + speaker_embedding.unsqueeze(1).expand(-1, max_src_len, -1)\n",
    "                + emotion_embedding.unsqueeze(1).expand(-1, max_src_len, -1)\n",
    "            )\n",
    "\n",
    "        output_dict = {\n",
    "            \"predicted_pitch\": var_adaptor_output[\"pitch_prediction\"],\n",
    "            \"predicted_energy\": var_adaptor_output[\"energy_prediction\"],\n",
    "            \"predicted_egemap\": var_adaptor_output[\"egemap_prediction\"],\n",
    "            \"predicted_log_durations\": var_adaptor_output[\"log_duration_prediction\"],\n",
    "            \"predicted_durations_rounded\": var_adaptor_output[\"duration_rounded\"],\n",
    "            \"emotion_embedding\": emotion_embedding,\n",
    "            \"encoder_attention\": encoder_attention,\n",
    "            \"decoder_attention\": decoder_attention,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
