{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gtts in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (2.5.4)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lanqingcui/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "#change the directory to run successfully at different end\n",
    "os.chdir('/Users/lanqingcui/Desktop')\n",
    "\n",
    "def text_to_speech(text, filename=\"output.mp3\", lang='en', slow=False):\n",
    "    \"\"\"Converts the given text to speech and saves it as an MP3 file with a custom name.\"\"\"\n",
    "    tts = gTTS(text=text, lang=lang, slow=slow)\n",
    "    tts.save(filename)\n",
    "    os.system(f\"afplay {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text_to_speech(\"Hello World! Welcome to our demonstration of text to speech. This is one simple example of easiest TTS. \", \"greeting.mp3\", lang='en',slow=True)\n",
    "    text_to_speech(\"Hello World! Welcome to our demonstration of text to speech. This is one simple example of easiest TTS. \", \"greeting2.mp3\", lang='en',slow=False)\n",
    "    text_to_speech(\"大家好呀！这是中文版测试，你今天过的怎么样? \", \"chinese_greeting.mp3\", lang='zh-CN',slow=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AI models for generating emotions: \n",
    "\n",
    "Find Dataset: \n",
    "    Publicly Available Emotional Speech Dataset (ESD) for Speech Synthesis and Voice Conversion\n",
    "       Dataset contains 10 English speakers with 5 emotional states (neutral, happy, angry, sad and surprise).\n",
    "\n",
    "Preprocess data and do training\n",
    "    Record necessary informations and normalize data\n",
    "    Sample Method:\n",
    "        1) extracts phones and feature from file\n",
    "        2) computes and saves:\n",
    "            2.1 pitch\n",
    "            2.2 mel-spectrogram\n",
    "            2.3 energy\n",
    "            2.4 durations\n",
    "        3) write in the data and paths\n",
    "            :param basename: str, filename without extension\n",
    "            :param tg_path: Path, path to .TextGrid file\n",
    "            :param wav_path: Path, path to .wav file\n",
    "            :param txt_path: Path, path to .txt file\n",
    "\n",
    "Impliment model and train the model on the data\n",
    "    FastSpeech2\n",
    "\n",
    "Check the Results of generation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Read and trim wav files\n",
    "        wav = torchaudio.load(wav_path)[0].cpu().numpy().squeeze(0)\n",
    "        wav = wav[\n",
    "            int(self.config.sample_rate * start) : int(self.config.sample_rate * end)\n",
    "        ].astype(np.float32)\n",
    "        speaker_idx, filename_idx, emotion_idx = basename.split(\"_\")\n",
    "\n",
    "\n",
    "# Compute pitch for certain audio\n",
    "        pitch, t = pw.dio( #Distributed Inline-filter Operation (DIO) algorithm.\n",
    "            wav.astype(np.float64),\n",
    "            self.config.sample_rate,\n",
    "            frame_period=self.config.hop_in_ms * 1000,\n",
    "        )\n",
    "        #Refined Pitch Estimation\n",
    "        pitch = pw.stonemask(wav.astype(np.float64), pitch, t, self.config.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paths(audio_save_path):\n",
    "    original_audio_path = Path(audio_save_path) / Path(\"original\")\n",
    "    logger.info(f\"Path for original audios: {original_audio_path}\")\n",
    "    generated_audio_path = Path(audio_save_path) / Path(\"generated\")\n",
    "    logger.info(f\"Path for generated audios: {generated_audio_path}\")\n",
    "    return original_audio_path, generated_audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSpeech2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FastSpeech2, self).__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.n_emotions = config.n_emotions\n",
    "\n",
    "        self.variance_adaptor = VarianceAdaptor(config)\n",
    "\n",
    "        self.emotion_emb = nn.Embedding(\n",
    "            config.n_emotions, config.emotion_emb_hidden_size\n",
    "        )\n",
    "        self.mel_linear = nn.Linear(\n",
    "            config.transformer_decoder_hidden, config.n_mel_channels\n",
    "        )\n",
    "        self.speaker_emb = nn.Embedding(\n",
    "            config.n_speakers + 1, config.speaker_emb_hidden_size\n",
    "        )\n",
    "\n",
    "        # Advanced Emotion Conditioning\n",
    "        self.conditional_cross_attention = config.conditional_cross_attention\n",
    "        self.conditional_layer_norm_usage = config.conditional_layer_norm\n",
    "        self.stack_speaker_with_emotion_embedding = (\n",
    "            config.stack_speaker_with_emotion_embedding\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, device, batch_dict) -> dict:\n",
    "\n",
    "        # Retrieves emotion and speaker embeddings based on the IDs\n",
    "        emotion_embedding = self.emotion_emb(batch_dict[\"emotions\"].to(device))\n",
    "        speaker_embedding = self.speaker_emb(batch_dict[\"speakers\"].to(device))\n",
    "\n",
    "        #if there is need of Conditional Processing\n",
    "        if self.conditional_cross_attention or self.conditional_layer_norm_usage:\n",
    "            encoder_output, encoder_attention = self.encoder(\n",
    "                batch_dict[\"texts\"].to(device),\n",
    "                src_masks.to(device),\n",
    "                speaker_emotion_embedding=emotion_embedding,\n",
    "            )\n",
    "        else:\n",
    "            encoder_output, encoder_attention = self.encoder(\n",
    "                batch_dict[\"texts\"].to(device),\n",
    "                src_masks.to(device),\n",
    "                speaker_emotion_embedding=None,\n",
    "            )\n",
    "\n",
    "        #checks if the speaker and emotion embeddings are not meant to be stacked together before being added to the encoder output.\n",
    "        if not self.stack_speaker_with_emotion_embedding:\n",
    "            max_src_len = torch.max(batch_dict[\"text_lens\"]).item()\n",
    "            encoder_output = (\n",
    "                encoder_output\n",
    "                + speaker_embedding.unsqueeze(1).expand(-1, max_src_len, -1)\n",
    "                + emotion_embedding.unsqueeze(1).expand(-1, max_src_len, -1)\n",
    "            )\n",
    "\n",
    "\n",
    "        # The VarianceAdaptor is used to process the encoder output and adapt features like pitch and duration based on the speech characteristics.\n",
    "        \n",
    "        # Decoder takes the output from the variance adaptor and produces the final mel spectrogram output\n",
    "       \n",
    "        #A linear transformation layer that maps the decoder's output to the target number of mel channels, finalizing the mel spectrogram generation.\n",
    "        \n",
    "        output_dict = {\n",
    "            \"predicted_pitch\": var_adaptor_output[\"pitch_prediction\"],\n",
    "            \"predicted_energy\": var_adaptor_output[\"energy_prediction\"],\n",
    "            \"predicted_egemap\": var_adaptor_output[\"egemap_prediction\"],\n",
    "            \"predicted_log_durations\": var_adaptor_output[\"log_duration_prediction\"],\n",
    "            \"predicted_durations_rounded\": var_adaptor_output[\"duration_rounded\"],\n",
    "            \"emotion_embedding\": emotion_embedding,\n",
    "            \"encoder_attention\": encoder_attention,\n",
    "            \"decoder_attention\": decoder_attention,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
